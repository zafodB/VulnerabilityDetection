import requests
import json
import datetime
from dateutil.parser import parse
import time
import os
from bs4 import BeautifulSoup


# keywords_location = "keywords.txt"
# data_location = "D:/Downloads/Tweets/pastebin/test/"
keywords_location = "/storagemount/data/pastebin_keywords/keywords.txt"
data_location = "/storagemount/data/pastebin_keywords/"


def search_ids(keyword_list):
    raw_ids = set()

    while keyword_list:
        try:

            time.sleep(3)
            search_term = keyword_list.pop()

            print("Fetching IDs for following keyword: " + search_term)

            response_ids = requests.get("https://psbdmp.ws/api/v2/search/" + search_term)
            collected_on = datetime.datetime.now().strftime("%Y%m%d%H%M%S")

            if response_ids.status_code != 200:
                keyword_list.append(search_term)
                print("Request not successful. Status code: " + str(response_ids.status_code))
                continue

            with open(os.path.join(id_location, ("pasted_ids_" + collected_on + ".json")), 'w+',
                      encoding='utf8') as file:
                file.write(response_ids.text)

            try:
                response_data = json.loads(response_ids.text)

            except ValueError as error:
                print("Could not parse JSON: " + str(error))
                keyword_list.append(search_term)
                continue

            for row in response_data['data']:
                raw_ids.add(row['id'])

        except KeyboardInterrupt as ke:
            with open(os.path.join(id_location, "all_ids.txt"), 'a+', encoding='utf8') as file:
                for paste_id in raw_ids:
                    file.write(paste_id)
            raise SystemExit(0)

    return raw_ids


def get_paste_texts(raw_ids: set):
    while raw_ids:
        try:
            time.sleep(5)
            paste_id = raw_ids.pop()

            print("Fetching raw paste with ID " + paste_id)
            pastebin_response = requests.get("https://pastebin.com/" + paste_id)
            collected_on = datetime.datetime.now().isoformat()

            if pastebin_response.status_code == 404:
                print("Request not successful. Status code: " + str(pastebin_response.status_code))
                continue
            elif pastebin_response.status_code != 200:
                raw_ids.add(paste_id)
                print("Request not successful. Status code: " + str(pastebin_response.status_code))
                continue

            else:

                pastebin_html = pastebin_response.text

                soup = BeautifulSoup(pastebin_html, 'html.parser')

                raw_paste_text = soup.find("textarea", {"id": "paste_code"}).contents[0]
                paste_title = soup.find("div", {"class": "paste_box_line1"}).findChild().text
                paste_author = soup.find("div", {"class": "paste_box_line2"}).contents[2].replace('\n', '').strip()
                paste_views = soup.find("div", {"class": "paste_box_line2"}).contents[8].replace('\n', '').replace(',', '').strip()
                paste_date = parse(soup.find("div", {"class": "paste_box_line2"}).findChild("span").get_text().strip())

                with open(os.path.join(pastes_location, (paste_id + ".json")), 'w+', encoding='utf8') as file:
                    json.dump({
                        'title': paste_title,
                        'pasteId': paste_id,
                        'pasteDate': paste_date.isoformat(),
                        'collectedDate': collected_on,
                        'pasteAuthor': paste_author,
                        'pasteViews': paste_views,
                        'pasteText': raw_paste_text
                    }, file)

        except KeyboardInterrupt as ke:
            with open(os.path.join(pastes_location, "remaining.txt"), 'w+', encoding='utf8') as file:
                for raw_id in raw_ids:
                    file.write(raw_id + '\n')

            raise SystemExit(0)


def load_all_ids() -> set:
    raw_ids = set()

    for root, dir, files in os.walk(id_location):
        for filename in files:
            with open(os.path.join(root, filename), 'r', encoding='utf8') as file:
                contents = json.load(file)

                for row in contents['data']:
                    raw_ids.add(row['id'])

    return raw_ids


def load_remaining_ids() -> set:
    raw_ids = set()
    with open(os.path.join(pastes_location, 'remaining.txt'), 'r', encoding='utf8') as file:
        all_ids = file.read()

    for i in range(len(all_ids) // 8):
        raw_ids.add(all_ids[i*8:(i+1)*8])

    return raw_ids


# Create dirs to store data
id_location = os.path.join(data_location, 'ids')
if not os.path.exists(id_location):
    os.mkdir(id_location)

pastes_location = os.path.join(data_location, 'pastes')
if not os.path.exists(pastes_location):
    os.mkdir(pastes_location)

# Read keywords from file
keywords = []
with open(keywords_location, 'r', encoding='utf8') as file:
    for line in file:
        keywords.append(line.replace('\n', ''))

# paste_ids = search_ids(keywords)
paste_ids = load_all_ids()

print("Finished querying keywords. Total collected IDs: " + str(len(paste_ids)))

get_paste_texts(paste_ids)

print("Successfully fetched all pastes.")


