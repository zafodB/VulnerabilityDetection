import os

import keras.backend.tensorflow_backend as tfback
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Lambda, Permute, LSTM, Conv1D, MaxPooling1D, Concatenate
from tensorflow.keras.optimizers import Adam
from tqdm import tqdm


def initial_setup():

    # Setup path for Graphviz plotting tool
    os.environ["PATH"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'

    # some random Keras bug as per: https://github.com/keras-team/keras/issues/13684
    def _get_available_gpus():
        """Get a list of available gpu devices (formatted as strings).

        # Returns
            A list of available GPU devices.
        """
        global _LOCAL_DEVICES
        if tfback._LOCAL_DEVICES is None:
            devices = tf.config.list_logical_devices()
            tfback._LOCAL_DEVICES = [x.name for x in devices]
        return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]

    tfback._get_available_gpus = _get_available_gpus
    tfback._get_available_gpus()


def load_numpy_data(data_directory: str, numpy_file_location: str):

    file_location = os.path.split(numpy_file_location)[-1]
    file_location = os.path.join(data_directory, 'numpys', file_location)

    return np.load(file=file_location)


def load_data(data_directory: str, data_file: str) -> tf.data.Dataset:

    tqdm.pandas()

    df = pd.read_csv(filepath_or_buffer=os.path.join(data_directory, data_file),
                     delimiter=';', header=0, converters={'encoded_target': eval})

    df['encoded_text_array'] = df['encoded_text_location'].progress_apply(lambda x: load_numpy_data(data_directory, x))

    target = df.pop(item='encoded_target')
    target = target.apply(lambda x: np.array(x)[np.newaxis])

    df.pop(item='raw_text')
    df.pop(item='raw_target')
    df.pop(item='encoded_text_location')

    dataset = tf.data.Dataset.from_tensor_slices(([value[0] for value in df.values], [value[0] for value in target.values]))

    batched_set = dataset.batch(1)
    return batched_set


initial_setup()

print("Starting up")


def build_model(days, posts, embedding_len, words, target_len):

    # Convolution over words in the post
    post_highlights_filters = 40
    post_highlights_kernel = 4
    post_highlights_stride = 1

    # Maxpool of the post
    post_maxpool_size = 10
    post_maxpool_stride = 5

    day_highlights_filters = 40
    day_highlights_kernel = 1
    day_highlights_stride = 1

    day_maxpool_size = 5
    day_maxpool_stride = 3
    lstm_units = 50

    input_shape = Input(shape=(days, posts, words, embedding_len))

    '''
    Choose 'relu' as activation function, as it supposedly performs fine. Will try other functions as necessary, if
    the network does not learn anything.
    
    Resources: https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0
    and: https://ai.stackexchange.com/questions/7088/how-to-choose-an-activation-function
    and: https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/
    '''
    post_highlights_convolution = Conv1D(filters=post_highlights_filters, kernel_size=post_highlights_kernel,
                                         strides=post_highlights_stride, activation='relu',
                                         name='post_highlights_convolution')
    post_permute = Permute(dims=(2,1))
    shared_post_maxpool = MaxPooling1D(pool_size=post_maxpool_size, strides=post_maxpool_stride,
                                       data_format='channels_first', name='shared_post_maxpool')

    day_highlights_convolution = Conv1D(filters=day_highlights_filters, kernel_size=day_highlights_kernel,
                                        strides=day_highlights_stride, activation='relu',
                                        name='day_highlights_convolution')

    shared_day_maxpool = MaxPooling1D(pool_size=day_maxpool_size, strides=day_maxpool_stride,
                                      data_format='channels_first', name='shared_day_maxpool')

    day_outputs = []

    for i in range(days):
        # Slice the input into different days. Take the second dimension, because first one on the left is 'batch_size'
        day = Lambda(lambda x: x[:, i, :, :, :])(input_shape)

        post_outputs = []

        for j in range(posts):
            # Slice the input into different posts. Take the second dimension,
            # because first one on the left is 'batch_size'. What is left is [words:embeddings]
            out = Lambda(lambda x: x[:, j, :, :])(day)

            post_x = (post_highlights_convolution)(out)
            post_x = (post_permute)(post_x)
            post_x = (shared_post_maxpool)(post_x)
            post_outputs.append(post_x)

        day_x = Concatenate()(post_outputs)
        day_x = (day_highlights_convolution)(day_x)
        day_x = (shared_day_maxpool)(day_x)
        day_outputs.append(day_x)

    week_overview = Concatenate()(day_outputs)

    output_x = LSTM(units=lstm_units)(week_overview)

    '''
    activation='sigmoid' is important here because of the multi-label classification problem.
    '''
    output_x = Dense(units=target_len, activation='sigmoid')(output_x)
    model = Model(inputs=input_shape, outputs=output_x)

    '''
    optimizer=Adam is sort of arbitrary choice, but was recommended as well-performing.
    
    Check here: https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/
    and here: https://datascience.stackexchange.com/questions/10523/guidelines-for-selecting-an-optimizer-for-training-neural-networks
    
    Recommended
    
    
    loss='binary_crossentropy' is important here because of the multi-label classification problem.
    
    Check here: https://stackoverflow.com/questions/44164749/how-does-keras-handle-multilabel-classification
    and here: https://towardsdatascience.com/multi-label-image-classification-with-neural-network-keras-ddc1ab1afede
    '''
    model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])

    print(model.summary())

    return model

# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)


# DATA SHAPE
days = 7
posts_per_day = 3
embedding_length = 768
max_words_per_post = 375
target_vector_length = 50

model = build_model(days, posts_per_day, embedding_length, max_words_per_post, target_vector_length)

data_directory = 'C:/Users/filip/Documents/thesis/data/series2'
data_file = 'train_data1.csv'
train_dataset = load_data(data_directory, data_file)

model.fit(train_dataset, epochs=10)
