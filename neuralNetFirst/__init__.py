import os

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Lambda, Permute, LSTM, Conv1D, MaxPooling1D, Concatenate, LeakyReLU
from tensorflow.keras.optimizers import Adam
from tqdm import tqdm


def initial_setup():
    import keras.backend.tensorflow_backend as tfback

    # Setup path for Graphviz plotting tool
    os.environ["PATH"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'

    # some random Keras bug as per: https://github.com/keras-team/keras/issues/13684
    def _get_available_gpus():
        """Get a list of available gpu devices (formatted as strings).

        # Returns
            A list of available GPU devices.
        """
        global _LOCAL_DEVICES
        if tfback._LOCAL_DEVICES is None:
            devices = tf.config.list_logical_devices()
            tfback._LOCAL_DEVICES = [x.name for x in devices]
        return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]

    tfback._get_available_gpus = _get_available_gpus
    tfback._get_available_gpus()


def build_model(days: int, posts: int, embedding_len: int, words: int, target_len: int, threshold: float=None) -> Model:
    """
    Build Keras model using Functional API. Model consists of three sub modules: Post Sub Model, Day Sub Model and
    Fortnight Sub Model.

    :param days: Number of days in each row. Determines the number of Day Sub Models
    :param posts: Number of posts in each day. Determines the number of Post Sub Models
    :param embedding_len: Length of embedding of a single post
    :param words: Number of words per one post, important to specify the input shape
    :param target_len: Length of the target vector. Determines the number of units in the last, dense layer.
    :return: Returns a ready Keras model with empty weights
    """

    # Post sub model parameters
    # Convolution
    post_highlights_filters = 80
    post_highlights_kernel = 5
    post_highlights_stride = 1

    # Maxpool
    post_maxpool_size = 10
    post_maxpool_stride = 5

    # Day sub model parameters
    # Convolution
    day_highlights_filters = 80
    day_highlights_kernel = 3
    day_highlights_stride = 1

    # Maxpool
    day_maxpool_size = 5
    day_maxpool_stride = 3

    # Fortnight sub model parameters
    lstm_units = 150

    input_shape = Input(shape=(days, posts, words, embedding_len))

    '''
    Choose 'relu' as activation function, as it supposedly performs fine. Will try other functions as necessary, if
    the network does not learn anything.

    Resources: https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0
    and: https://ai.stackexchange.com/questions/7088/how-to-choose-an-activation-function
    and: https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/
    '''
    post_highlights_convolution = Conv1D(filters=post_highlights_filters, kernel_size=post_highlights_kernel,
                                         strides=post_highlights_stride, activation=LeakyReLU(),
                                         name='post_highlights_convolution')
    post_permute = Permute(dims=(2, 1))
    shared_post_maxpool = MaxPooling1D(pool_size=post_maxpool_size, strides=post_maxpool_stride,
                                       data_format='channels_first', name='shared_post_maxpool')

    day_highlights_convolution = Conv1D(filters=day_highlights_filters, kernel_size=day_highlights_kernel,
                                        strides=day_highlights_stride, activation=LeakyReLU(),
                                        name='day_highlights_convolution')

    shared_day_maxpool = MaxPooling1D(pool_size=day_maxpool_size, strides=day_maxpool_stride,
                                      data_format='channels_first', name='shared_day_maxpool')

    day_outputs = []

    for i in range(days):
        # Slice the input into different days. Take the second dimension, because first one on the left is 'batch_size'
        day = Lambda(lambda x: x[:, i, :, :, :])(input_shape)

        post_outputs = []

        for j in range(posts):
            # Slice the input into different posts. Take the second dimension,
            # because first one on the left is 'batch_size'. What is left is [words:embeddings]
            out = Lambda(lambda x: x[:, j, :, :])(day)

            post_x = (post_highlights_convolution)(out)
            post_x = (post_permute)(post_x)
            post_x = (shared_post_maxpool)(post_x)
            post_outputs.append(post_x)

        day_x = Concatenate()(post_outputs)
        day_x = (day_highlights_convolution)(day_x)
        day_x = (shared_day_maxpool)(day_x)
        day_outputs.append(day_x)

    week_overview = Concatenate()(day_outputs)

    output_x = LSTM(units=lstm_units)(week_overview)

    '''
    activation='sigmoid' is important here because of the multi-label classification problem.
    '''
    output_x = Dense(units=target_len, activation='sigmoid')(output_x)
    model = Model(inputs=input_shape, outputs=output_x)

    '''
    optimizer=Adam is sort of arbitrary choice, but was recommended as well-performing.

    Check here: https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/
    and here: https://datascience.stackexchange.com/questions/10523/guidelines-for-selecting-an-optimizer-for-training-neural-networks

    loss='binary_crossentropy' is important here because of the multi-label classification problem.

    Check here: https://stackoverflow.com/questions/44164749/how-does-keras-handle-multilabel-classification
    and here: https://towardsdatascience.com/multi-label-image-classification-with-neural-network-keras-ddc1ab1afede
    '''

    if threshold:
        recall_metric = tf.keras.metrics.Recall(thresholds=threshold)
    else:
        recall_metric = tf.keras.metrics.Recall()

    model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy',
                  metrics=[recall_metric, tf.keras.metrics.BinaryAccuracy()])

    print(model.summary())

    return model


def load_numpy_data(data_directory: str, numpy_file_location: str, subfolder_name: str = 'numpys') -> np.array:
    """
    Open up and load a numpy file from disk. Used during data loading process.

    :param data_directory: Base directory where all data is stored (e.g. /raid/data/series4)
    :param numpy_file_location: File location as extracted from the dataframe.
    :param subfolder_name: Subfolder name where numpy files are stored.
    :return: Numpy array read from file.
    """

    # Currently, this data location is written to the dataframe as full path on disk. This might become inaccurate
    # if the file is moved on disk.
    file_location = os.path.split(numpy_file_location)[-1]
    file_location = os.path.join(data_directory, subfolder_name, file_location)

    return np.load(file=file_location)


def load_data(data_directory: str, features_shape: tuple, target_vector_length: int, data_file: str = 'train_data1.csv',
              n_rows=None, batches=None, prefetch=None) -> tf.data.Dataset:
    """
    Load data from a CSV file and a bunch of numpy files, produced by dataprep/create_training_rows.py

    :param data_directory: Directory where the CSV file and the numpy sub-directory are located
    :param features_shape: Dimensionality of the input space. Must include days, posts per day, embedding length and
    max number of words per post
    :param target_vector_length: Number of classes to predict.
    :param data_file: Name of the CSV file. If None, default is used.
    :param n_rows: Number rows that should be retrieved at random from the dataframe. If none, all rows are retrieved.
    :param batches: Rows per batch, if batching is used. If None, batches are not produced
    :param prefetch: Number of items that are prefetched. If None, prefetching is not used
    :return: Tensorflow dataset
    """
    tqdm.pandas()

    df = pd.read_csv(filepath_or_buffer=os.path.join(data_directory, data_file),
                     delimiter=';', header=0, converters={'encoded_target': eval})

    if n_rows:
        df = df.sample(n=n_rows)

#    pd.set_option('display.max_colwidth', None)

#   print(df['raw_text'])

#    return

    df.pop(item='raw_text')
    df.pop(item='raw_target')

    print('Loading numpy arrays.')
    df['encoded_text_array'] = df['encoded_text_location'].progress_apply(lambda x: load_numpy_data(data_directory, x))
    df['encoded_target_ready'] = df['encoded_target'].apply(lambda x: np.array(x)[np.newaxis])

    print("Number of rows: " + str(df.shape))

    df.pop('encoded_text_location')

    def df_generator():
        for i, row in df.iterrows():
            features = row['encoded_text_array']
            label = row['encoded_target_ready'][0]

            yield features, label

    print('Constructing data from generator.')
    dataset = tf.data.Dataset.from_generator(df_generator, output_types=(tf.float32, tf.int32),
                                             output_shapes=(features_shape, (target_vector_length)))

    if batches:
        dataset = dataset.batch(batches)

    if prefetch:
        dataset = dataset.prefetch(prefetch)

    print("Data loaded.")
    return dataset
