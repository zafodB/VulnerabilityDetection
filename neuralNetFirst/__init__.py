import os

import tensorflow as tf
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Lambda, Permute, LSTM, Conv1D, MaxPooling1D, Concatenate
from tensorflow.keras.optimizers import Adam
import numpy as np
import pandas as pd
import tensorflow as tf
from tqdm import tqdm


def initial_setup():
    import keras.backend.tensorflow_backend as tfback

    # Setup path for Graphviz plotting tool
    os.environ["PATH"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'

    # some random Keras bug as per: https://github.com/keras-team/keras/issues/13684
    def _get_available_gpus():
        """Get a list of available gpu devices (formatted as strings).

        # Returns
            A list of available GPU devices.
        """
        global _LOCAL_DEVICES
        if tfback._LOCAL_DEVICES is None:
            devices = tf.config.list_logical_devices()
            tfback._LOCAL_DEVICES = [x.name for x in devices]
        return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]

    tfback._get_available_gpus = _get_available_gpus
    tfback._get_available_gpus()


def build_model(days, posts, embedding_len, words, target_len):
    # Convolution over words in the post
    post_highlights_filters = 40
    post_highlights_kernel = 4
    post_highlights_stride = 1

    # Maxpool of the post
    post_maxpool_size = 10
    post_maxpool_stride = 5

    day_highlights_filters = 40
    day_highlights_kernel = 1
    day_highlights_stride = 1

    day_maxpool_size = 5
    day_maxpool_stride = 3
    lstm_units = 50

    input_shape = Input(shape=(days, posts, words, embedding_len))

    '''
    Choose 'relu' as activation function, as it supposedly performs fine. Will try other functions as necessary, if
    the network does not learn anything.

    Resources: https://towardsdatascience.com/visualizing-intermediate-activation-in-convolutional-neural-networks-with-keras-260b36d60d0
    and: https://ai.stackexchange.com/questions/7088/how-to-choose-an-activation-function
    and: https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/
    '''
    post_highlights_convolution = Conv1D(filters=post_highlights_filters, kernel_size=post_highlights_kernel,
                                         strides=post_highlights_stride, activation='relu',
                                         name='post_highlights_convolution')
    post_permute = Permute(dims=(2, 1))
    shared_post_maxpool = MaxPooling1D(pool_size=post_maxpool_size, strides=post_maxpool_stride,
                                       data_format='channels_first', name='shared_post_maxpool')

    day_highlights_convolution = Conv1D(filters=day_highlights_filters, kernel_size=day_highlights_kernel,
                                        strides=day_highlights_stride, activation='relu',
                                        name='day_highlights_convolution')

    shared_day_maxpool = MaxPooling1D(pool_size=day_maxpool_size, strides=day_maxpool_stride,
                                      data_format='channels_first', name='shared_day_maxpool')

    day_outputs = []

    for i in range(days):
        # Slice the input into different days. Take the second dimension, because first one on the left is 'batch_size'
        day = Lambda(lambda x: x[:, i, :, :, :])(input_shape)

        post_outputs = []

        for j in range(posts):
            # Slice the input into different posts. Take the second dimension,
            # because first one on the left is 'batch_size'. What is left is [words:embeddings]
            out = Lambda(lambda x: x[:, j, :, :])(day)

            post_x = (post_highlights_convolution)(out)
            post_x = (post_permute)(post_x)
            post_x = (shared_post_maxpool)(post_x)
            post_outputs.append(post_x)

        day_x = Concatenate()(post_outputs)
        day_x = (day_highlights_convolution)(day_x)
        day_x = (shared_day_maxpool)(day_x)
        day_outputs.append(day_x)

    week_overview = Concatenate()(day_outputs)

    output_x = LSTM(units=lstm_units)(week_overview)

    '''
    activation='sigmoid' is important here because of the multi-label classification problem.
    '''
    output_x = Dense(units=target_len, activation='sigmoid')(output_x)
    model = Model(inputs=input_shape, outputs=output_x)

    '''
    optimizer=Adam is sort of arbitrary choice, but was recommended as well-performing.

    Check here: https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/
    and here: https://datascience.stackexchange.com/questions/10523/guidelines-for-selecting-an-optimizer-for-training-neural-networks

    Recommended


    loss='binary_crossentropy' is important here because of the multi-label classification problem.

    Check here: https://stackoverflow.com/questions/44164749/how-does-keras-handle-multilabel-classification
    and here: https://towardsdatascience.com/multi-label-image-classification-with-neural-network-keras-ddc1ab1afede
    '''
    model.compile(optimizer=Adam(learning_rate=0.1), loss='binary_crossentropy',
                  metrics=[tf.keras.metrics.Recall(), tf.keras.metrics.Accuracy()])

    print(model.summary())

    return model


def load_numpy_data(data_directory: str, numpy_file_location: str):

    file_location = os.path.split(numpy_file_location)[-1]
    file_location = os.path.join(data_directory, 'numpys', file_location)

    return np.load(file=file_location)


def load_data(data_directory: str, data_file: str, features_shape: tuple, target_vector_length: int, top_n_rows=None) -> tf.data.Dataset:
    tqdm.pandas()

    df = pd.read_csv(filepath_or_buffer=os.path.join(data_directory, data_file),
                     delimiter=';', header=0, converters={'encoded_target': eval})

    if top_n_rows:
        data_length = len(df)
        df.drop(df.tail(data_length - top_n_rows).index, inplace=True)  # drop last n rows

    df.pop(item='raw_text')
    df.pop(item='raw_target')

    print('Loading numpy arrays.')
    df['encoded_text_array'] = df['encoded_text'].progress_apply(lambda x: load_numpy_data(data_directory, x))
    df['encoded_target_ready'] = df['encoded_target'].apply(lambda x: np.array(x)[np.newaxis])

    print("Number of rows: " + str(df.shape))

    df.pop('encoded_text')

    def df_generator():
        for i, row in df.iterrows():
            features = row['encoded_text_array']
            label = row['encoded_target_ready'][0]

            yield features, label

    print('Constructing data from generator.')
    dataset = tf.data.Dataset.from_generator(df_generator, output_types=(tf.float32, tf.int32),
                                             output_shapes=(features_shape, (target_vector_length)))

    dataset = dataset.batch(10)
    dataset = dataset.prefetch(2)

    print("Data loaded.")
    return dataset
