import os

import keras.backend.tensorflow_backend as tfback
import pandas as pd
import tensorflow as tf
# from tensorflow.keras.layers import LSTM
# from tensorflow.keras.layers.convolutional import Conv1D, MaxPooling1D
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense, Lambda, Permute, LSTM, Conv1D, MaxPooling1D, Concatenate
# from tensorflow.keras.layers.merge import Concatenate
from tensorflow.keras.optimizers import Adam


def stupid_setup():

    # Setup path for Graphviz plotting tool
    os.environ["PATH"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'

    # some random Keras bug as per: https://github.com/keras-team/keras/issues/13684
    def _get_available_gpus():
        """Get a list of available gpu devices (formatted as strings).

        # Returns
            A list of available GPU devices.
        """
        #global _LOCAL_DEVICES
        if tfback._LOCAL_DEVICES is None:
            devices = tf.config.list_logical_devices()
            tfback._LOCAL_DEVICES = [x.name for x in devices]
        return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]
    tfback._get_available_gpus = _get_available_gpus
    tfback._get_available_gpus()


def load_data(data_location: str) -> tf.data.Dataset:
    df = pd.read_csv(filepath_or_buffer=data_location, delimiter=';', header=0)

    target = df.pop(item='encoded_target')

    df.pop(item='raw_text')
    df.pop(item='raw_target')

    print(df.head(5))
    print(df.dtypes)

    dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))

    # for feat, targ in dataset.take(5):
    #     print('Features: {}, Target: {}'.format(feat, targ))

    return dataset


stupid_setup()

print("hello there")

# sys.exit(0)

days = 31
embedding_length = 768
max_words_per_sentence = 25
max_sentences_per_post = 200
# actual_words_per_post = None # (Unknown)
# posts_per_day = None # (Unknown)
posts_per_day = 50

predicted_vul_count = 50

shared_post_conv = Conv1D(filters=40, kernel_size=4, strides=1)
shared_permute = Permute((2,1))
shared_post_maxpool = MaxPooling1D(pool_size=10, strides=5, data_format='channels_first')

shared_day_conv = Conv1D(filters=40, kernel_size=1, strides=1)
shared_day_maxpool = MaxPooling1D(pool_size=5, strides=3, data_format='channels_first')

input = Input(shape=(days, posts_per_day, max_sentences_per_post, max_words_per_sentence, embedding_length))

print(input.shape)
day_outputs = []

for i in range(days):
    day = Lambda(lambda x: x[:, i, :, :, :])(input)

    post_outputs = []

    for j in range(posts_per_day):
        out = Lambda(lambda x: x[:, j, :, :])(day)
        # print("After lambda: " + str(out.shape))

        post_x = (shared_post_conv)(out)
        # print("After conv: " + str(post_x.shape))

        post_x = (shared_permute)(post_x)
        # print("After permute: " + str(post_x.shape))

        post_x = (shared_post_maxpool)(post_x)

        post_outputs.append(post_x)

    day_x = Concatenate()(post_outputs)
    # print("After merge: " + str(merge_posts.shape))

    day_x = (shared_day_conv)(day_x)

    # print("After daily conv: " + str(day_x.shape))
    # x2 = (shared_permute)(x1)
    day_x = (shared_day_maxpool)(day_x)
    day_outputs.append(day_x)

week_overview = Concatenate()(day_outputs)

output_x = LSTM(units=50)(week_overview)

'''
activation='sigmoid' is important here because of the multi-label classification problem.
'''
output_x = Dense(units=predicted_vul_count, activation='sigmoid')(output_x)

model = Model(inputs=input, outputs=output_x)

'''
loss='binary_crossentropy' is important here because of the multi-label classification problem.

Check here: https://stackoverflow.com/questions/44164749/how-does-keras-handle-multilabel-classification
and here: https://towardsdatascience.com/multi-label-image-classification-with-neural-network-keras-ddc1ab1afede
'''
model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy')

# x_train = None
# paded_input = pad_sequences(None, padding='post')


# print(model.summary())
# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

# train_dataset = load_data('D:/Downloads/data/train_data1.csv').batch(1)

# model.fit(train_dataset, epochs=2)
