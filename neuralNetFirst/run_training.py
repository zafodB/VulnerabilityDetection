import os

import numpy as np
import pandas as pd
import tensorflow as tf
from tqdm import tqdm

from neuralNetFirst import build_model, initial_setup
from neuralNetFirst.nn_config import *


def load_numpy_data(data_directory: str, numpy_file_location: str):

    file_location = os.path.split(numpy_file_location)[-1]
    file_location = os.path.join(data_directory, 'numpys', file_location)

    return np.load(file=file_location)


def load_data(data_directory: str, data_file: str, features_shape: tuple, target_vector_length: int) -> tf.data.Dataset:
    tqdm.pandas()

    df = pd.read_csv(filepath_or_buffer=os.path.join(data_directory, data_file),
                     delimiter=';', header=0, converters={'encoded_target': eval})
    
    df.pop(item='raw_text')
    df.pop(item='raw_target')

    print('Loading numpy arrays.')
    df['encoded_text_array'] = df['encoded_text'].progress_apply(lambda x: load_numpy_data(data_directory, x))
    df['encoded_target_ready'] = df['encoded_target'].apply(lambda x: np.array(x)[np.newaxis])

    print("Number of rows: " + str(df.shape))

    df.pop('encoded_text')

    def df_generator():
        for i, row in df.iterrows():
            features = row['encoded_text_array']
            label = row['encoded_target_ready'][0]

            yield features, label

    print('Constructing data from generator.')
    dataset = tf.data.Dataset.from_generator(df_generator, output_types=(tf.float32, tf.int32),
                                             output_shapes=(features_shape, (target_vector_length)))

    dataset = dataset.batch(10)
    dataset = dataset.prefetch(2)

    print("Data loaded.")
    return dataset


# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

if RUN_GPU_CONFIG:
    initial_setup()

print("Starting up")

features_shape = (DAYS, POSTS_PER_DAY, MAX_WORDS_PER_POST, EMBEDDING_LENGTH)

model = build_model(DAYS, POSTS_PER_DAY, EMBEDDING_LENGTH, MAX_WORDS_PER_POST, TARGET_VECTOR_LENGTH)


train_dataset = load_data(data_directory_path, data_file, features_shape, TARGET_VECTOR_LENGTH)

model.fit(train_dataset, epochs=10)

if not os.path.isdir(model_directory_path):
    os.mkdir(model_directory_path)

model.save_weights(os.path.join(model_directory_path, 'model_weights'), overwrite=True)
