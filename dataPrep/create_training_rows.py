import json
import os
import pandas as pd
from datetime import timedelta
from datetime import datetime
import pytz
import ast
import re
import nltk
import numpy as np
import sys
import uuid

from bert_serving.client import BertClient

# SERVER PARAMETERS (set on server)
MAX_SEQUENCE_LEN = 25

# DIMENSIONS
number_weeks = 5
vulnerabilities_per_week = 3
rows_per_vulnerability = 2
days_per_row = 7
posts_per_day = 2

# INPUT
vulnerability_file = '/datadrive/train-data/series1/top50vuls.json'

# OUTPUT
output_file = '/datadrive/train-data/series1/train_data1.csv'
output_numpy_location = '/datadrive/train-data/series1/numpys'

# PARAMETERS
year = '2016'
max_post_sentences = 15
RANDOMNESS_SEED = 156487
posts_dtypes = {
    'IdPost': 'float64',
    'Author': 'int64',
    'Thread': 'object',
    'Timestamp': 'object',
    'Content': 'object',
    'AuthorNumPosts': 'float64',
    'AuthorReputation': 'float64',
    'LastParse': 'object',
    'parsed': 'object',
    'Site': 'float64',
    'CitedPost': 'object',
    'AuthorName': 'object',
    'Likes': 'float64',
}
column_names = ('raw_text', 'encoded_text_location', 'raw_target', 'encoded_target')

# This nltk download should be run once, first time when this code is run.
# nltk.download('punkt')


def print_completion_time():
    print("\nCompleted on: " + str(datetime.now()))


def load_json_file(filename: str) -> dict:
    with open(filename, 'r', encoding='utf8') as cve_import_file:
        return json.load(cve_import_file)


def jprint(json_to_print: dict):
    print('\n' + json.dumps(json_to_print, indent=2))


def find_cpe23uri_in_dict(input: dict):
    """
    Recursively retrieve all identifiers of affected software (cpe32uri) from a dict.

    Example cpe32uri: cpe:2.3:a:microsoft:.net_framework:1.1:sp1:*:*:*:*:*:*

    :param input: NVD database JSON dump.
    :return: list of exctracted cpe32uris, if any. None otherwise.
    """
    output = []

    if isinstance(input, dict):
        if 'cpe23Uri' in input and 'vulnerable' in input and input['vulnerable'] == True:
            output.append(input['cpe23Uri'])

        else:

            for key, value in input.items():
                result = find_cpe23uri_in_dict(value)

                if result:
                    for item in result:
                        output.append(item)
                        # print(item)

    elif isinstance(input, list):
        for listitem in input:
            result = find_cpe23uri_in_dict(listitem)

            if result:
                for resultitem in result:
                    output.append(resultitem)

    else:
        return None

    return output


def save_cpe32uri_raw(cve_list: dict) -> dict:
    """
    Save all cpe32uri strings to dict

    :param cve_list: CVE list, as imported from NVD json file.
    :return: Dictionary mapping vulnerability strings to CVE numbers. Structure:
        'cve-id1':
            {'vulnerable_config_raw':
                ['cpe32uri_1', 'cpe32uri_2', 'cpe32uri_3', ...]
            }
        'cve-id2':
            ...
    """
    print("Saving cpe32uris.")

    cve_config_list = {}
    for cve in cve_list['CVE_Items']:

        vulnerable_stuff = []
        for node in cve['configurations']['nodes']:
            for item in find_cpe23uri_in_dict(node):
                vulnerable_stuff.append(item)

        cve_id = cve['cve']['CVE_data_meta']['ID']

        cve_config_list[cve_id] = {'vulnerable_config_raw': vulnerable_stuff}

    return cve_config_list


def filter_vendor_and_software(cve_config_raw: list) -> dict:
    """
    Filter out vendor and product from dictionary with cpe32uris and count different affected versions.

    :param cve_config_raw: mapping of cpe32uri to cve-id numbers. Output of 'save_cpe32uri_raw`

    :return: Ammended input dictionary with filtered counts.
    """
    # print("Filtering out vendors and software.")

    config_counts = {}
    for config in cve_config_raw:

        split_config = config.split(':')
        config_name = split_config[3] + ' ' + split_config[4]

        if config_name not in config_counts:
            config_counts[config_name] = 1
        else:
            config_counts[config_name] += 1

    return config_counts


def split_long_sentences(long_sentences: list) -> list:
    """
    Take sentences that are longer than parameter MAX_SEQUENCE_LEN and split them into
    shorter chunks as necessary.

    :param long_sentences: list of sentences, some of which may be too long
    :return: changed list of sentences, where all sentences are of good length. Order of the words of the input
    is preserved.
    """

    problem_sentences = {}
    for index, sentence in enumerate(long_sentences):
        words = re.findall(r'\w+', sentence)

        if len(words) > MAX_SEQUENCE_LEN:
            problem_sentences[index] = []

            while len(words) > MAX_SEQUENCE_LEN:
                problem_sentences[index].append(' '.join(words[:MAX_SEQUENCE_LEN]))
                words = words[MAX_SEQUENCE_LEN:]

            problem_sentences[index].append(' '.join(words))

    if len(problem_sentences.keys()) > 0:

        order = sorted(list(problem_sentences.keys()), reverse=True)

        for index in order:
            long_sentences.pop(index)
            for short_sentence in reversed(problem_sentences[index]):
                long_sentences.insert(index, short_sentence)

    return long_sentences


def embed_post(post_text: str, max_number_sentences: int):
    """
    Embed post text into BERT embeddings. Post text is first split into sentences of maximum length equal or
    less than MAX_SENTENCE_LEN. Then individual embedding of length 768 is obtained for each word. Every sentence
    is padded with 0 until MAX_SENTENCE_LEN is reached.

    If post has more sentences than allowed max_number_sentences, the excessive sentences are removed. If post has
    fewer than max_number_sentences, the array is padded with zeros until max_number_sentences is reached.

    :param post_text: raw post text from forum
    :param max_number_sentences: maximum allowed number of sentences in a post.
    :return: numpy.ndarray with dimensions [(max_number_sentences * MAX_SENTENCE_LEN)][768]

    """

    post_text = post_text.replace('\n', ' ').replace('\t', ' ')

    sentences = nltk.tokenize.sent_tokenize(post_text)
    sentences = split_long_sentences(sentences)

    if len(sentences) > max_number_sentences:
        print('\nTrimming post')
        sentences = sentences[:max_number_sentences]

    space_to_pad = (max_number_sentences - len(sentences)) * MAX_SEQUENCE_LEN

    encoded_sentences = np.concatenate(bc.encode(sentences, show_tokens=False))

    if len(sentences) < max_number_sentences:
        encoded_sentences = np.pad(array=encoded_sentences, mode='constant', pad_width=((0, space_to_pad), (0, 0)),
                                   constant_values=0)

    return encoded_sentences


def encode_vulnerabilities(affected_software: list, most_affected_software) -> list:
    """
    Encode vulnerability target as one-hot vector (multiple ones are allowed)

    :param affected_software: list of software affected in the given row
    :param most_affected_software: list of most affected software (length of this list = length of target vector)
    :return: multiple-hot encoded vector (like one-hot, but multiple 1 allowed)
    """
    output = []
    for vul in most_affected_software:
        if vul in affected_software:
            output.append(1)
        else:
            output.append(0)

    return output


def load_vulnerabilities(file_location: str) -> pd.DataFrame:
    """
    Load vulnerabilities from csv to Dataframe. Prepare some additional columns.

    :param file_location: Location of csv file with vulnerabilities. The file was constructed in notebook cve_explore
    and should contain the following columns:

        date, cve, configurations, impact, publishedDate, lastModifiedDate, id

    :return: Dataframe with vulnerabilities
    """
    with open(file_location, 'r', encoding='utf8') as file:
        top_vulnerabilities = json.load(file)

    df = pd.read_csv('/datadrive/train-data/2016_filtered.csv')

    df['date'] = pd.to_datetime(df['date'], utc=True, infer_datetime_format=True)
    df['week_no'] = df['date'].apply(lambda x: datetime.date(x).isocalendar()[1])
    df['vul-target'] = df['configurations'].apply(lambda x: list(
        filter_vendor_and_software(find_cpe23uri_in_dict(ast.literal_eval(x))).keys()))
    df['one-hot'] = df['vul-target'].apply(lambda x: encode_vulnerabilities(x, top_vulnerabilities))

    return df


bc = BertClient()

# Load vulnerabilities to iterate through
vulnerabilities = load_vulnerabilities(vulnerability_file)

# Create a placeholder file to store training data when ready.
training_data = pd.DataFrame(columns=column_names)
training_data.to_csv(path_or_buf=output_file, sep=';', header=True, index=False, mode='w+')

'''
Iterate to create training data.

Iterate in this order:

weeks
    vulnerabilities
        training rows
            week days
                number of posts per day
                    random post

First iterate through weeks, starting from week 2 (no vulnerabilities in week 1).
    Then iterate through vulnerabilities in that week.
        For each vulnerability in that week, create a specified number of training rows (where posts are random and
        different, but target vector is the same,
            In each row, iterate over days of the week.
                For each weekday, find a specified number of posts.
                    For each post number, select a random post that was created on that day and remove it from the list.
                    Encode the text of this post and append the encoded and original version to the list.
'''
# There are no vulnerabilities published in week 1, start from week 2.
for week_no in range(2, number_weeks + 1):

    print("Now processing week: " + str(week_no) + '. Reading posts csv file.')

    posts = pd.read_csv(
        filepath_or_buffer=('/datadrive/train-data/posts_weeks/2016_posts_week_' + str(week_no) + '.csv'),
        dtype=posts_dtypes, sep=';', parse_dates=['conv_time'], header=0)
    posts = posts.dropna(subset=['week_no'])
    posts = posts.astype({'week_no': 'int32'})

    assert posts.index.is_unique

    for number_vulnerabilities, vulnerability in enumerate(
            vulnerabilities[vulnerabilities['week_no'] == week_no]
                    .sample(frac=1, random_state=RANDOMNESS_SEED)
                    .iterrows()):

        if number_vulnerabilities > vulnerabilities_per_week:
            break

        print("Processing vulnerability number: " + str(number_vulnerabilities))

        for k in range(rows_per_vulnerability):
            print('\tCreating training row: ' + str(k))

            vul_target = vulnerability[1]['vul-target']
            vul_onehot = vulnerability[1]['one-hot']

            post_raw_texts = []
            post_embedded_texts = []
            for week_day in range(1, days_per_row + 1):
                print('\t\tProcessing day: ' + str(week_day) + '. Posts found: ', end='')

                daily_raw_texts = []
                daily_embedded_text = []

                for l in range(posts_per_day):
                    print(' ' + str(l), end='')

                    desired_post_date = pytz.utc.localize(datetime.strptime(
                        (year + '-W' + str(week_no) + '-' + str(week_day)), '%G-W%V-%u'))

                    try:
                        random_post = posts[(posts['conv_time'] > desired_post_date) & (
                                posts['conv_time'] <= (desired_post_date + timedelta(days=1)))].sample()
                    except ValueError as va:
                        print('No more posts in that day!')
                        raise va

                    posts.drop(random_post.index)

                    post_text = random_post['Content'].values[0]

                    daily_raw_texts.append(post_text)
                    daily_embedded_text.append(embed_post(post_text=post_text, max_number_sentences=max_post_sentences))

                print()
                post_raw_texts.append(daily_raw_texts)
                post_embedded_texts.append(np.stack(daily_embedded_text))

            stacked_embedded_text = np.stack(arrays=post_embedded_texts)

            filename = str(uuid.uuid4()) + '.npy'
            numpy_file_location = os.path.join(output_numpy_location, filename)
            np.save(numpy_file_location, stacked_embedded_text)
            print('Array with dimensions: ' + str(stacked_embedded_text.shape) + ' saved to .npy file.')

            data_list = {'0': [post_raw_texts, numpy_file_location, vul_target, vul_onehot]}
            data_row = pd.DataFrame.from_dict(data_list, orient='index', columns=column_names)

            training_data = training_data.append(data_row, ignore_index=True)

            print("Appended new training data row. Training data shape: " + str(training_data.shape))

        print('Writing out data to file.')

        training_data.to_csv(path_or_buf=output_file, sep=';', header=False, index=False, mode='a')
        training_data = pd.DataFrame(columns=column_names)

"""
produced -> (week, vulnerability, training examples/rows, posts, sentences, words, embeddings)

week is not really a dimension, only divided into weeks because of the way how data is stored on disk.

saved -> (vulnerability, training examples/rows, posts, sentences, words, embeddings)

the same, when loaded into a dataframe:
    1 training example = 1 dataframe row 
                    -> (posts x [sentences x [words x embeddings] ], vulnerability (one-hot) )
    
should in fact be (ideally): 
                    -> (days x [posts x [sentences x [words x embeddings] ], vulnerability (one-hot) )

should in fact be (in practice): 
sentences are removed, words are served continuously
                    -> (days x [posts x [(sentences x words) x embeddings] ], vulnerability (one-hot) )
"""
