import json
import os
import pandas as pd
from datetime import timedelta
from datetime import datetime
import pytz
import ast
import re
import nltk

from bert_serving.client import BertClient

# This parameter is set on server
MAX_SEQUENCE_LEN = 25
bc = BertClient()

# nltk.download('punkt')


def print_completion_time():
    print("\nCompleted on: " + str(datetime.now()))


def load_json_file(filename: str) -> dict:
    with open(filename, 'r', encoding='utf8') as cve_import_file:
        return json.load(cve_import_file)


def jprint(json_to_print: dict):
    print('\n' + json.dumps(json_to_print, indent=2))


def find_cpe23uri_in_dict(input: dict):
    """
    Recursively retrieve all identifiers of affected software (cpe32uri) from a dict.

    Example cpe32uri: cpe:2.3:a:microsoft:.net_framework:1.1:sp1:*:*:*:*:*:*

    :param input: NVD database JSON dump.
    :return: list of exctracted cpe32uris, if any. None otherwise.
    """
    output = []

    if isinstance(input, dict):
        if 'cpe23Uri' in input and 'vulnerable' in input and input['vulnerable'] == True:
            output.append(input['cpe23Uri'])

        else:

            for key, value in input.items():
                result = find_cpe23uri_in_dict(value)

                if result:
                    for item in result:
                        output.append(item)
                        # print(item)

    elif isinstance(input, list):
        for listitem in input:
            result = find_cpe23uri_in_dict(listitem)

            if result:
                for resultitem in result:
                    output.append(resultitem)

    else:
        return None

    return output


def save_cpe32uri_raw(cve_list: dict) -> dict:
    """
    Save all cpe32uri strings to dict

    :param cve_list: CVE list, as imported from NVD json file.
    :return: Dictionary mapping vulnerability strings to CVE numbers. Structure:
        'cve-id1':
            {'vulnerable_config_raw':
                ['cpe32uri_1', 'cpe32uri_2', 'cpe32uri_3', ...]
            }
        'cve-id2':
            ...
    """
    print("Saving cpe32uris.")

    cve_config_list = {}
    for cve in cve_list['CVE_Items']:

        vulnerable_stuff = []
        for node in cve['configurations']['nodes']:
            for item in find_cpe23uri_in_dict(node):
                vulnerable_stuff.append(item)

        cve_id = cve['cve']['CVE_data_meta']['ID']

        cve_config_list[cve_id] ={'vulnerable_config_raw' : vulnerable_stuff}

    return cve_config_list


def filter_vendor_and_software(cve_config_raw: list) -> dict:
    """
    Filter out vendor and product from dictionary with cpe32uris and count different affected versions.

    :param cve_config_raw: mapping of cpe32uri to cve-id numbers. Output of 'save_cpe32uri_raw`

    :return: Ammended input dictionary with filtered counts.
    """
    # print("Filtering out vendors and software.")

    config_counts = {}
    for config in cve_config_raw:

        split_config = config.split(':')
        config_name = split_config[3] + ' ' + split_config[4]

        if config_name not in config_counts:
            config_counts[config_name] = 1
        else:
            config_counts[config_name] += 1

    return config_counts


def split_long_sentences(long_sentences: list) -> list:
    """
    Take sentences that are longer than parameter MAX_SEQUENCE_LEN and split them into
    shorter chunks as necessary.

    :param long_sentences: list of sentences, some of which may be too long
    :return: changed list of sentences, where all sentences are of good length. Order of the words of the input
    is preserved.
    """

    problem_sentences = {}
    for index, sentence in enumerate(long_sentences):
        words = re.findall(r'\w+', sentence)

        if len(words) > MAX_SEQUENCE_LEN:
            problem_sentences[index] = []

            while len(words) > MAX_SEQUENCE_LEN:
                problem_sentences[index].append(' '.join(words[:MAX_SEQUENCE_LEN]))
                words = words[MAX_SEQUENCE_LEN:]

            problem_sentences[index].append(' '.join(words))

    if len(problem_sentences.keys()) > 0:

        order = sorted(list(problem_sentences.keys()), reverse=True)

        for index in order:
            long_sentences.pop(index)
            for short_sentence in reversed(problem_sentences[index]):
                long_sentences.insert(index, short_sentence)

    return long_sentences


def embed_post(post_text: str):
    """
    Embed post text into BERT embeddings. Post text is first split into sentences of maximum length equal or
    less than MAX_SENTENCE_LEN. Then individual embedding of length 768 is obtained for each word. Every sentence
    is padded with 0 until MAX_SENTENCE_LEN is reached.

    :param post_text: raw post text from forum
    :return: numpy.ndarray with dimensions [number of sentences][MAX_SENTENCE_LEN][768]

    """

    post_text = post_text.replace('\n', ' ').replace('\t', ' ')

    sentences = nltk.tokenize.sent_tokenize(post_text)
    sentences = split_long_sentences(sentences)

    # result = bc.encode(sentences, show_tokens=True)
    encoded_sentences = bc.encode(sentences, show_tokens=False)

    return encoded_sentences


def encode_vulnerabilities(input: list) -> list:
    output = []
    for vul in top_vulnerabilities:
        if vul in input:
            output.append(1)
        else:
            output.append(0)

    return output


with open('/datadrive/train-data/series1/top50vuls.json', 'r', encoding='utf8') as file:
    top_vulnerabilities = json.load(file)

df_cve = pd.read_csv('/datadrive/train-data/2016_filtered.csv')

df_cve['date'] = pd.to_datetime(df_cve['date'], utc=True, infer_datetime_format=True)

# # df_cve['vul-']
for col in df_cve.columns:
    print(col)
print(df_cve.head(5))

df_cve['week_no'] = df_cve['date'].apply(lambda x: datetime.date(x).isocalendar()[1])

df_cve['vul-target'] = df_cve['configurations'].apply(lambda x: list(
    filter_vendor_and_software(find_cpe23uri_in_dict(ast.literal_eval(x))).keys()))


df_cve['one-hot'] = df_cve['vul-target'].apply(lambda x: encode_vulnerabilities(x))

number_weeks = 53
vulnerabiliies_per_week = 10
rows_per_vulnerability = 5
posts_per_row = 30

RANDOMNESS_SEED = 156487
output_file = '/datadrive/train-data/series1/train_data1.csv'

#
column_names = ['raw_text', 'encoded_text', 'raw_target', 'encoded_target']
training_data = pd.DataFrame(columns=column_names)

training_data.to_csv(path_or_buf=output_file, sep=';', header=True, index=False, mode='w+')


for i in range(1, number_weeks + 1):

    print("Now processing week: " + str(i) + '. Reading posts csv file.')

    posts = pd.read_csv(filepath_or_buffer=('/datadrive/train-data/posts_weeks/2016_posts_week_'+str(i)+'.csv') , sep=';', header=0)

    posts = posts.dropna(subset=['week_no'])
    posts = posts.astype({'week_no': 'int32'})

    # There are no vulnerabilities that were published in week 1 (?)
    if i == 1:
        week_no = 2
    else:
        week_no = i

    number_vulnerabilities = 0
    for vulnerability in df_cve[df_cve['week_no'] == week_no].sample(frac=1, random_state=RANDOMNESS_SEED).iterrows():
        number_vulnerabilities += 1
        if number_vulnerabilities > vulnerabiliies_per_week:
            break

        print("Processing vulnerability number: " + str(number_vulnerabilities))

        for j in range(rows_per_vulnerability):

            vul_target = vulnerability[1]['vul-target']
            vul_onehot = vulnerability[1]['one-hot']

            post_texts = []
            embedded_text = []

            for k in range(posts_per_row):

                random_post = posts.sample()

                post_text = random_post['Content'].values[0]

                post_texts.append(post_text)
                embedded_text.append(embed_post(post_text))

            data_list = {'0': [post_texts, embedded_text, vul_target, vul_onehot]}
            data_row = pd.DataFrame.from_dict(data_list, orient='index', columns=column_names)


            training_data = training_data.append(data_row, ignore_index=True)

            print("Appendded new training data row. Training data shape: " + str(training_data.shape))

        print('Writing out data to file.')
        training_data.to_csv(path_or_buf=output_file, sep=';', header=False, index=False, mode='a')
        training_data = pd.DataFrame(columns=column_names)
