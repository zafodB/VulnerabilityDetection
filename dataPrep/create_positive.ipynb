{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Initial setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed on: 2020-06-06 18:27:44.125450\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "# This parameter is set on server\n",
    "MAX_SEQUENCE_LEN = 25\n",
    "bc = BertClient()\n",
    "\n",
    "# The nltk download should be run once initially.\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def print_completion_time():\n",
    "    print(\"\\nCompleted on: \" + str(datetime.now()))\n",
    "\n",
    "\n",
    "def load_json_file(filename: str) -> dict:\n",
    "    with open(filename, 'r', encoding='utf8') as cve_import_file:\n",
    "        return json.load(cve_import_file)\n",
    "\n",
    "\n",
    "def jprint(json_to_print: dict):\n",
    "    print('\\n' + json.dumps(json_to_print, indent=2))\n",
    "\n",
    "\n",
    "def find_cpe23uri_in_dict(input: dict):\n",
    "    \"\"\"\n",
    "    Recursively retrieve all identifiers of affected software (cpe32uri) from a dict.\n",
    "\n",
    "    Example cpe32uri: cpe:2.3:a:microsoft:.net_framework:1.1:sp1:*:*:*:*:*:*\n",
    "\n",
    "    :param input: NVD database JSON dump.\n",
    "    :return: list of exctracted cpe32uris, if any. None otherwise.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "\n",
    "    if isinstance(input, dict):\n",
    "        if 'cpe23Uri' in input and 'vulnerable' in input and input['vulnerable'] == True:\n",
    "            output.append(input['cpe23Uri'])\n",
    "\n",
    "        else:\n",
    "\n",
    "            for key, value in input.items():\n",
    "                result = find_cpe23uri_in_dict(value)\n",
    "\n",
    "                if result:\n",
    "                    for item in result:\n",
    "                        output.append(item)\n",
    "                        # print(item)\n",
    "\n",
    "    elif isinstance(input, list):\n",
    "        for listitem in input:\n",
    "            result = find_cpe23uri_in_dict(listitem)\n",
    "\n",
    "            if result:\n",
    "                for resultitem in result:\n",
    "                    output.append(resultitem)\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def save_cpe32uri_raw(cve_list: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Save all cpe32uri strings to dict\n",
    "\n",
    "    :param cve_list: CVE list, as imported from NVD json file.\n",
    "    :return: Dictionary mapping vulnerability strings to CVE numbers. Structure:\n",
    "        'cve-id1':\n",
    "            {'vulnerable_config_raw':\n",
    "                ['cpe32uri_1', 'cpe32uri_2', 'cpe32uri_3', ...]\n",
    "            }\n",
    "        'cve-id2':\n",
    "            ...\n",
    "    \"\"\"\n",
    "    print(\"Saving cpe32uris.\")\n",
    "\n",
    "    cve_config_list = {}\n",
    "    for cve in cve_list['CVE_Items']:\n",
    "\n",
    "        vulnerable_stuff = []\n",
    "        for node in cve['configurations']['nodes']:\n",
    "            for item in find_cpe23uri_in_dict(node):\n",
    "                vulnerable_stuff.append(item)\n",
    "\n",
    "        cve_id = cve['cve']['CVE_data_meta']['ID']\n",
    "\n",
    "        cve_config_list[cve_id] ={'vulnerable_config_raw' : vulnerable_stuff}\n",
    "\n",
    "    return cve_config_list\n",
    "\n",
    "\n",
    "def filter_vendor_and_software(cve_config_raw: list) -> dict:\n",
    "    \"\"\"\n",
    "    Filter out vendor and product from dictionary with cpe32uris and count different affected versions.\n",
    "\n",
    "    :param cve_config_raw: mapping of cpe32uri to cve-id numbers. Output of 'save_cpe32uri_raw`\n",
    "\n",
    "    :return: Ammended input dictionary with filtered counts.\n",
    "    \"\"\"\n",
    "\n",
    "    config_counts = {}\n",
    "    for config in cve_config_raw:\n",
    "\n",
    "        split_config = config.split(':')\n",
    "        config_name = split_config[3] + ' ' + split_config[4]\n",
    "\n",
    "        if config_name not in config_counts:\n",
    "            config_counts[config_name] = 1\n",
    "        else:\n",
    "            config_counts[config_name] += 1\n",
    "\n",
    "    return config_counts\n",
    "\n",
    "\n",
    "def split_long_sentences(long_sentences: list) -> list:\n",
    "    \"\"\"\n",
    "    Take sentences that are longer than parameter MAX_SEQUENCE_LEN and split them into\n",
    "    shorter chunks as necessary.\n",
    "\n",
    "    :param long_sentences: list of sentences, some of which may be too long\n",
    "    :return: changed list of sentences, where all sentences are of good length. Order of the words of the input\n",
    "    is preserved.\n",
    "    \"\"\"\n",
    "\n",
    "    problem_sentences = {}\n",
    "    for index, sentence in enumerate(long_sentences):\n",
    "        words = re.findall(r'\\w+', sentence)\n",
    "\n",
    "        if len(words) > MAX_SEQUENCE_LEN:\n",
    "            problem_sentences[index] = []\n",
    "\n",
    "            while len(words) > MAX_SEQUENCE_LEN:\n",
    "                problem_sentences[index].append(' '.join(words[:MAX_SEQUENCE_LEN]))\n",
    "                words = words[MAX_SEQUENCE_LEN:]\n",
    "\n",
    "            problem_sentences[index].append(' '.join(words))\n",
    "\n",
    "    if len(problem_sentences.keys()) > 0:\n",
    "\n",
    "        order = sorted(list(problem_sentences.keys()), reverse=True)\n",
    "\n",
    "        for index in order:\n",
    "            long_sentences.pop(index)\n",
    "            for short_sentence in reversed(problem_sentences[index]):\n",
    "                long_sentences.insert(index, short_sentence)\n",
    "\n",
    "    return long_sentences\n",
    "\n",
    "\n",
    "def embed_post(post_text: str, max_number_sentences: int):\n",
    "    \"\"\"\n",
    "    Embed post text into BERT embeddings. Post text is first split into sentences of maximum length equal or\n",
    "    less than MAX_SENTENCE_LEN. Then individual embedding of length 768 is obtained for each word. Every sentence\n",
    "    is padded with 0 until MAX_SENTENCE_LEN is reached.\n",
    "\n",
    "    If post has more sentences than allowed max_number_sentences, the excessive sentences are removed. If post has\n",
    "    fewer than max_number_sentences, the array is padded with zeros until max_number_sentences is reached.\n",
    "\n",
    "    :param post_text: raw post text from forum\n",
    "    :param max_number_sentences: maximum allowed number of sentences in a post.\n",
    "    :return: numpy.ndarray with dimensions [(max_number_sentences * MAX_SENTENCE_LEN)][768]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    post_text = post_text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "    sentences = nltk.tokenize.sent_tokenize(post_text)\n",
    "    sentences = split_long_sentences(sentences)\n",
    "\n",
    "    if len(sentences) > max_number_sentences:\n",
    "        print('\\nTrimming post')\n",
    "        sentences = sentences[:max_number_sentences]\n",
    "\n",
    "    space_to_pad = (max_number_sentences - len(sentences)) * MAX_SEQUENCE_LEN\n",
    "\n",
    "    encoded_sentences = np.concatenate(bc.encode(sentences, show_tokens=False))\n",
    "\n",
    "    if len(sentences) < max_number_sentences:\n",
    "        encoded_sentences = np.pad(array=encoded_sentences, mode='constant', pad_width=((0, space_to_pad), (0, 0)), constant_values=0)\n",
    "\n",
    "    return encoded_sentences\n",
    "\n",
    "print_completion_time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create\n",
    "Create positive examples by combining posts with CVE ids from the same time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n",
      "cve\n",
      "configurations\n",
      "impact\n",
      "publishedDate\n",
      "lastModifiedDate\n",
      "id\n",
      "                       date  \\\n",
      "0 2016-01-13 05:59:00+00:00   \n",
      "1 2016-01-13 05:59:00+00:00   \n",
      "2 2016-01-13 05:59:00+00:00   \n",
      "3 2016-01-13 05:59:00+00:00   \n",
      "4 2016-01-13 05:59:00+00:00   \n",
      "\n",
      "                                                 cve  \\\n",
      "0  {'data_type': 'CVE', 'data_format': 'MITRE', '...   \n",
      "1  {'data_type': 'CVE', 'data_format': 'MITRE', '...   \n",
      "2  {'data_type': 'CVE', 'data_format': 'MITRE', '...   \n",
      "3  {'data_type': 'CVE', 'data_format': 'MITRE', '...   \n",
      "4  {'data_type': 'CVE', 'data_format': 'MITRE', '...   \n",
      "\n",
      "                                      configurations  \\\n",
      "0  {'CVE_data_version': '4.0', 'nodes': [{'operat...   \n",
      "1  {'CVE_data_version': '4.0', 'nodes': [{'operat...   \n",
      "2  {'CVE_data_version': '4.0', 'nodes': [{'operat...   \n",
      "3  {'CVE_data_version': '4.0', 'nodes': [{'operat...   \n",
      "4  {'CVE_data_version': '4.0', 'nodes': [{'operat...   \n",
      "\n",
      "                                              impact      publishedDate  \\\n",
      "0  {'baseMetricV3': {'cvssV3': {'version': '3.0',...  2016-01-13T05:59Z   \n",
      "1  {'baseMetricV3': {'cvssV3': {'version': '3.0',...  2016-01-13T05:59Z   \n",
      "2  {'baseMetricV3': {'cvssV3': {'version': '3.0',...  2016-01-13T05:59Z   \n",
      "3  {'baseMetricV3': {'cvssV3': {'version': '3.0',...  2016-01-13T05:59Z   \n",
      "4  {'baseMetricV3': {'cvssV3': {'version': '3.0',...  2016-01-13T05:59Z   \n",
      "\n",
      "    lastModifiedDate             id  \n",
      "0  2018-10-12T22:10Z  CVE-2016-0003  \n",
      "1  2018-10-12T22:10Z  CVE-2016-0005  \n",
      "2  2019-05-17T20:08Z  CVE-2016-0006  \n",
      "3  2019-05-17T20:17Z  CVE-2016-0007  \n",
      "4  2019-05-15T19:28Z  CVE-2016-0008  \n",
      "\n",
      "Completed on: 2020-06-06 18:27:48.750388\n"
     ]
    }
   ],
   "source": [
    "with open('/datadrive/train-data/series1/top50vuls.json', 'r', encoding='utf8') as file:\n",
    "    top_vulnerabilities = json.load(file)\n",
    "\n",
    "df_cve = pd.read_csv('/datadrive/train-data/2016_filtered.csv')\n",
    "\n",
    "df_cve['date'] = pd.to_datetime(df_cve['date'], utc=True, infer_datetime_format=True)\n",
    "\n",
    "for col in df_cve.columns:\n",
    "    print(col)\n",
    "print(df_cve.head(5))\n",
    "\n",
    "df_cve['week_no'] = df_cve['date'].apply(lambda x: datetime.date(x).isocalendar()[1])\n",
    "\n",
    "df_cve['vul-target'] = df_cve['configurations'].apply(lambda x: list(\n",
    "    filter_vendor_and_software(find_cpe23uri_in_dict(ast.literal_eval(x))).keys()))\n",
    "\n",
    "\n",
    "def encode_vulnerabilities(input: list) -> list:\n",
    "    output = []\n",
    "    for vul in top_vulnerabilities:\n",
    "        if vul in input:\n",
    "            output.append(1)\n",
    "        else:\n",
    "            output.append(0)\n",
    "\n",
    "    return output\n",
    "\n",
    "df_cve['one-hot'] = df_cve['vul-target'].apply(lambda x: encode_vulnerabilities(x))\n",
    "\n",
    "print_completion_time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Construct training rows"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing week: 2. Reading posts csv file.\n",
      "Processing vulnerability number: 0\n",
      "\tCreating training row: 0\n",
      "\t\tProcessing day: 1. Posts found:  0 1\n",
      "\t\tProcessing day: 2. Posts found:  0 1\n",
      "\t\tProcessing day: 3. Posts found:  0 1\n",
      "\t\tProcessing day: 4. Posts found:  0 1\n",
      "\t\tProcessing day: 5. Posts found:  0 1\n",
      "\t\tProcessing day: 6. Posts found:  0 1\n",
      "\t\tProcessing day: 7. Posts found:  0 1\n",
      "(7, 2, 375, 768)\n",
      "Appendded new training data row. Training data shape: (1, 4)\n",
      "Writing out data to file.\n",
      "Processing vulnerability number: 1\n",
      "\tCreating training row: 0\n",
      "\t\tProcessing day: 1. Posts found:  0 1\n",
      "\t\tProcessing day: 2. Posts found:  0 1\n",
      "\t\tProcessing day: 3. Posts found:  0 1\n",
      "\t\tProcessing day: 4. Posts found:  0 1\n",
      "\t\tProcessing day: 5. Posts found:  0 1\n",
      "\t\tProcessing day: 6. Posts found:  0 1\n",
      "\t\tProcessing day: 7. Posts found:  0 1\n",
      "(7, 2, 375, 768)\n",
      "Appendded new training data row. Training data shape: (1, 4)\n",
      "Writing out data to file.\n",
      "Processing vulnerability number: 2\n",
      "\tCreating training row: 0\n",
      "\t\tProcessing day: 1. Posts found:  0 1\n",
      "\t\tProcessing day: 2. Posts found:  0 1\n",
      "\t\tProcessing day: 3. Posts found:  0 1\n",
      "\t\tProcessing day: 4. Posts found:  0 1\n",
      "\t\tProcessing day: 5. Posts found:  0 1\n",
      "\t\tProcessing day: 6. Posts found:  0 1\n",
      "\t\tProcessing day: 7. Posts found:  0 1\n",
      "(7, 2, 375, 768)\n",
      "Appendded new training data row. Training data shape: (1, 4)\n",
      "Writing out data to file.\n",
      "Processing vulnerability number: 3\n",
      "\tCreating training row: 0\n",
      "\t\tProcessing day: 1. Posts found:  0 1\n",
      "\t\tProcessing day: 2. Posts found:  0 1\n",
      "\t\tProcessing day: 3. Posts found:  0 1\n",
      "\t\tProcessing day: 4. Posts found:  0 1\n",
      "\t\tProcessing day: 5. Posts found:  0 1\n",
      "\t\tProcessing day: 6. Posts found:  0 1\n",
      "\t\tProcessing day: 7. Posts found:  0 1\n",
      "(7, 2, 375, 768)\n",
      "Appendded new training data row. Training data shape: (1, 4)\n",
      "Writing out data to file.\n",
      "Now processing week: 3. Reading posts csv file.\n",
      "Processing vulnerability number: 0\n",
      "\tCreating training row: 0\n",
      "\t\tProcessing day: 1. Posts found:  0 1\n",
      "\t\tProcessing day: 2. Posts found:  0 1\n",
      "\t\tProcessing day: 3. Posts found:  0 1\n",
      "\t\tProcessing day: 4. Posts found:  0 1\n",
      "\t\tProcessing day: 5. Posts found:  0 1\n",
      "\t\tProcessing day: 6. Posts found:  0 1\n",
      "\t\tProcessing day: 7. Posts found:  0 1\n",
      "(7, 2, 375, 768)\n",
      "Appendded new training data row. Training data shape: (1, 4)\n",
      "Writing out data to file.\n",
      "Processing vulnerability number: 1\n",
      "\tCreating training row: 0\n",
      "\t\tProcessing day: 1. Posts found:  0 1\n",
      "\t\tProcessing day: 2. Posts found:  0 1\n",
      "\t\tProcessing day: 3. Posts found:  0 1\n",
      "\t\tProcessing day: 4. Posts found:  0 1\n",
      "\t\tProcessing day: 5. Posts found:  0 1\n",
      "\t\tProcessing day: 6. Posts found:  0 1\n",
      "\t\tProcessing day: 7. Posts found:  0 1\n",
      "(7, 2, 375, 768)\n",
      "Appendded new training data row. Training data shape: (1, 4)\n",
      "Writing out data to file.\n",
      "Processing vulnerability number: 2\n",
      "\tCreating training row: 0\n",
      "\t\tProcessing day: 1. Posts found:  0 1\n",
      "\t\tProcessing day: 2. Posts found:  0 1\n",
      "\t\tProcessing day: 3. Posts found:  0 1\n",
      "\t\tProcessing day: 4. Posts found:  0 1\n",
      "\t\tProcessing day: 5. Posts found:  0 1\n",
      "\t\tProcessing day: 6. Posts found:  0 1\n",
      "\t\tProcessing day: 7. Posts found:  0 1\n",
      "(7, 2, 375, 768)\n",
      "Appendded new training data row. Training data shape: (1, 4)\n",
      "Writing out data to file.\n",
      "Processing vulnerability number: 3\n",
      "\tCreating training row: 0\n",
      "\t\tProcessing day: 1. Posts found:  0 1\n",
      "\t\tProcessing day: 2. Posts found:  0 1\n",
      "\t\tProcessing day: 3. Posts found:  0 1\n",
      "\t\tProcessing day: 4. Posts found:  0 1\n",
      "\t\tProcessing day: 5. Posts found:  0 1\n",
      "\t\tProcessing day: 6. Posts found:  0 1\n",
      "\t\tProcessing day: 7. Posts found:  0 1\n",
      "(7, 2, 375, 768)\n",
      "Appendded new training data row. Training data shape: (1, 4)\n",
      "Writing out data to file.\n",
      "Now processing week: 4. Reading posts csv file.\n",
      "Processing vulnerability number: 0\n",
      "\tCreating training row: 0\n",
      "\t\tProcessing day: 1. Posts found:  0 1\n",
      "\t\tProcessing day: 2. Posts found:  0 1\n",
      "\t\tProcessing day: 3. Posts found:  0 1\n",
      "\t\tProcessing day: 4. Posts found:  0 1\n",
      "\t\tProcessing day: 5. Posts found:  0 1\n",
      "\t\tProcessing day: 6. Posts found:  0 1\n",
      "\t\tProcessing day: 7. Posts found:  0 1\n",
      "(7, 2, 375, 768)\n",
      "Appendded new training data row. Training data shape: (1, 4)\n",
      "Writing out data to file.\n",
      "Processing vulnerability number: 1\n",
      "\tCreating training row: 0\n",
      "\t\tProcessing day: 1. Posts found:  0 1\n",
      "\t\tProcessing day: 2. Posts found:  0 1\n",
      "\t\tProcessing day: 3. Posts found:  0 1\n",
      "\t\tProcessing day: 4. Posts found:  0 1\n",
      "\t\tProcessing day: 5. Posts found:  0 1\n",
      "\t\tProcessing day: 6. Posts found:  0 1\n",
      "\t\tProcessing day: 7. Posts found:  0 1\n",
      "(7, 2, 375, 768)\n",
      "Appendded new training data row. Training data shape: (1, 4)\n",
      "Writing out data to file.\n",
      "Processing vulnerability number: 2\n",
      "\tCreating training row: 0\n",
      "\t\tProcessing day: 1. Posts found:  0 1\n",
      "\t\tProcessing day: 2. Posts found:  0 1\n",
      "\t\tProcessing day: 3. Posts found:  0 1\n",
      "\t\tProcessing day: 4. Posts found:  0 1\n",
      "\t\tProcessing day: 5. Posts found:  0 1\n",
      "\t\tProcessing day: 6. Posts found:  0 1\n",
      "\t\tProcessing day: 7. Posts found:  0 1\n",
      "(7, 2, 375, 768)\n",
      "Appendded new training data row. Training data shape: (1, 4)\n",
      "Writing out data to file.\n",
      "Processing vulnerability number: 3\n",
      "\tCreating training row: 0\n",
      "\t\tProcessing day: 1. Posts found:  0 1\n",
      "\t\tProcessing day: 2. Posts found:  0 1\n",
      "\t\tProcessing day: 3. Posts found:  0 1\n",
      "\t\tProcessing day: 4. Posts found:  0 1\n",
      "\t\tProcessing day: 5. Posts found:  0 1\n",
      "\t\tProcessing day: 6. Posts found:  0Trimming\n",
      " 1\n",
      "\t\tProcessing day: 7. Posts found:  0 1\n",
      "(7, 2, 375, 768)\n",
      "Appendded new training data row. Training data shape: (1, 4)\n",
      "Writing out data to file.\n",
      "Now processing week: 5. Reading posts csv file.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-39eb2c993d2e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m     posts = pd.read_csv(filepath_or_buffer=('/datadrive/train-data/posts_weeks/2016_posts_week_'+str(week_no)+'.csv') ,\n\u001B[0;32m---> 44\u001B[0;31m                         dtype=posts_dtypes, sep=';', parse_dates=['conv_time'], header=0)\n\u001B[0m\u001B[1;32m     45\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[0mposts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mposts\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropna\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msubset\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'week_no'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/filipy/lib/python3.6/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mparser_f\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001B[0m\n\u001B[1;32m    674\u001B[0m         )\n\u001B[1;32m    675\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 676\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    677\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    678\u001B[0m     \u001B[0mparser_f\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/filipy/lib/python3.6/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    453\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 454\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mparser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    455\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    456\u001B[0m         \u001B[0mparser\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/filipy/lib/python3.6/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   1131\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1132\u001B[0m         \u001B[0mnrows\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_validate_integer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"nrows\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1133\u001B[0;31m         \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1134\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1135\u001B[0m         \u001B[0;31m# May alter columns / col_dict\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/filipy/lib/python3.6/site-packages/pandas/io/parsers.py\u001B[0m in \u001B[0;36mread\u001B[0;34m(self, nrows)\u001B[0m\n\u001B[1;32m   2035\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnrows\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2036\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2037\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnrows\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2038\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2039\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_first_chunk\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader.read\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32mpandas/_libs/parsers.pyx\u001B[0m in \u001B[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;32m~/filipy/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001B[0m in \u001B[0;36mis_extension_array_dtype\u001B[0;34m(arr_or_dtype)\u001B[0m\n\u001B[1;32m   1563\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1564\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1565\u001B[0;31m \u001B[0;32mdef\u001B[0m \u001B[0mis_extension_array_dtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marr_or_dtype\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1566\u001B[0m     \"\"\"\n\u001B[1;32m   1567\u001B[0m     \u001B[0mCheck\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0man\u001B[0m \u001B[0mobject\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0ma\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0mextension\u001B[0m \u001B[0marray\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "year = '2016'\n",
    "number_weeks = 53\n",
    "vulnerabiliies_per_week = 3\n",
    "rows_per_vulnerability = 1\n",
    "days_per_row = 7\n",
    "posts_per_day = 2\n",
    "\n",
    "max_post_sentences = 15\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "RANDOMNESS_SEED = 156487\n",
    "output_file = '/datadrive/train-data/series1/train_data1.csv'\n",
    "output_numpy_location = '/datadrive/train-data/series1/numpys'\n",
    "\n",
    "#\n",
    "column_names = ['raw_text', 'encoded_text', 'raw_target', 'encoded_target']\n",
    "training_data = pd.DataFrame(columns=column_names)\n",
    "\n",
    "training_data.to_csv(path_or_buf=output_file, sep=';', header=True, index=False, mode='w+')\n",
    "\n",
    "# There are no vulnerabilities that were published in week 1 (?)\n",
    "for week_no in range(2, number_weeks + 1):\n",
    "\n",
    "    print(\"Now processing week: \" + str(week_no) + '. Reading posts csv file.')\n",
    "    \n",
    "    posts_dtypes = {\n",
    "        'IdPost': 'float64',\n",
    "        'Author': 'int64',\n",
    "        'Thread': 'object',\n",
    "        'Timestamp': 'object',\n",
    "        'Content': 'object',\n",
    "        'AuthorNumPosts': 'float64',\n",
    "        'AuthorReputation': 'float64',\n",
    "        'LastParse': 'object',\n",
    "        'parsed': 'object',\n",
    "        'Site': 'float64',\n",
    "        'CitedPost': 'object',\n",
    "        'AuthorName': 'object',\n",
    "        'Likes': 'float64',\n",
    "    }\n",
    "\n",
    "    posts = pd.read_csv(filepath_or_buffer=('/datadrive/train-data/posts_weeks/2016_posts_week_'+str(week_no)+'.csv') ,\n",
    "                        dtype=posts_dtypes, sep=';', parse_dates=['conv_time'], header=0)\n",
    "\n",
    "    posts = posts.dropna(subset=['week_no'])\n",
    "    posts = posts.astype({'week_no': 'int32'})\n",
    "\n",
    "    assert posts.index.is_unique\n",
    "\n",
    "    for number_vulnerabilities, vulnerability in enumerate(df_cve[df_cve['week_no'] == week_no].sample(frac=1, random_state=RANDOMNESS_SEED).iterrows()):\n",
    "\n",
    "        if number_vulnerabilities > vulnerabiliies_per_week:\n",
    "            break\n",
    "\n",
    "        print(\"Processing vulnerability number: \" + str(number_vulnerabilities))\n",
    "\n",
    "        for k in range(rows_per_vulnerability):\n",
    "            print('\\tCreating training row: ' + str(k))\n",
    "\n",
    "            vul_target = vulnerability[1]['vul-target']\n",
    "            vul_onehot = vulnerability[1]['one-hot']\n",
    "\n",
    "            post_raw_texts = []\n",
    "            post_embedded_texts = []\n",
    "            for week_day in range(1, days_per_row + 1):\n",
    "                print('\\t\\tProcessing day: ' + str(week_day) + '. Posts found: ', end='')\n",
    "\n",
    "                daily_raw_texts = []\n",
    "                daily_embedded_text = []\n",
    "\n",
    "                for l in range(posts_per_day):\n",
    "                    print(' ' +str(l), end='')\n",
    "\n",
    "                    desired_post_date = pytz.utc.localize(datetime.strptime(\n",
    "                        (year + '-W' + str(week_no) + '-' + str(week_day)),'%G-W%V-%u'))\n",
    "\n",
    "                    try:\n",
    "                        random_post = posts[(posts['conv_time'] > desired_post_date) & (posts['conv_time'] <= (desired_post_date + timedelta(days=1)))].sample()\n",
    "                    except ValueError as va:\n",
    "                        print('No more posts in that day!')\n",
    "                        raise va\n",
    "\n",
    "                    posts.drop(random_post.index)\n",
    "\n",
    "                    post_text = random_post['Content'].values[0]\n",
    "\n",
    "                    daily_raw_texts.append(post_text)\n",
    "                    daily_embedded_text.append(embed_post(post_text=post_text, max_number_sentences=max_post_sentences))\n",
    "\n",
    "                print()\n",
    "                post_raw_texts.append(daily_raw_texts)\n",
    "                post_embedded_texts.append(np.stack(daily_embedded_text))\n",
    "\n",
    "            stacked_embedded_text = np.stack(arrays=post_embedded_texts)\n",
    "\n",
    "            filename = str(uuid.uuid4()) + '.npy'\n",
    "            numpy_file_location = os.path.join(output_numpy_location, filename)\n",
    "            np.save(numpy_file_location, stacked_embedded_text)\n",
    "            print('Array with dimensions: ' + str(stacked_embedded_text.shape) + ' saved to .npy file.')\n",
    "\n",
    "            data_list = {'0': [post_raw_texts, numpy_file_location, vul_target, vul_onehot]}\n",
    "            data_row = pd.DataFrame.from_dict(data_list, orient='index', columns=column_names)\n",
    "\n",
    "            training_data = training_data.append(data_row, ignore_index=True)\n",
    "\n",
    "            print(\"Appendded new training data row. Training data shape: \" + str(training_data.shape))\n",
    "\n",
    "        print('Writing out data to file.')\n",
    "\n",
    "        training_data.to_csv(path_or_buf=output_file, sep=';', header=False, index=False, mode='a')\n",
    "        training_data = pd.DataFrame(columns=column_names)\n",
    "\n",
    "print_completion_time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}