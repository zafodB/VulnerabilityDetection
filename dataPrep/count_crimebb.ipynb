{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "def print_completion_time():\n",
    "    \"\"\"\n",
    "    Print current time (In PyCharm, remote jupyter doesn't indicate when cell is finished computing.)\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print(\"\\nCompleted on: \" + str(datetime.now()))\n",
    "\n",
    "\n",
    "def load_json_file(filename: str) -> dict:\n",
    "    with open(filename, 'r', encoding='utf8') as cve_import_file:\n",
    "        return json.load(cve_import_file)\n",
    "\n",
    "\n",
    "def jprint(json_to_print: dict):\n",
    "    print('\\n' + json.dumps(json_to_print, indent=2))\n",
    "\n",
    "print_completion_time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Histogram of posts by year.\n",
    "\n",
    "Iteratively (in batches) read `posts.csv` file as dumped from PostgreSQL. Sumarise and save histogram to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "df_dates = pd.DataFrame()\n",
    "\n",
    "for chunk in pd.read_csv('/datadrive/posts.csv', sep=';', header=0, chunksize=1000000):\n",
    "    print(\"Now processing chunk \" + str(i))\n",
    "    i += 1\n",
    "\n",
    "    print(chunk['Timestamp'].min())\n",
    "    print(chunk['Timestamp'].max())\n",
    "\n",
    "    print('\\nConverting')\n",
    "    chunk = chunk.filter(['Timestamp'])\n",
    "\n",
    "    # Convert timestanp to datetime format, so Pandas can work with it.\n",
    "    chunk['conv_time']=pd.to_datetime(chunk['Timestamp'], utc=True, infer_datetime_format=True)\n",
    "    print('\\nAppending')\n",
    "\n",
    "    chunk = chunk.filter(['conv_time'])\n",
    "    df_dates = df_dates.append(chunk, ignore_index=True)\n",
    "\n",
    "    print(\"Length of the dataframe is: \" + str(df_dates.shape))\n",
    "\n",
    "df_datecounts = df_dates['conv_time'].groupby([df_dates[\"conv_time\"].dt.year]).count()\n",
    "\n",
    "ax = df_datecounts.plot(kind=\"bar\")\n",
    "\n",
    "for i in ax.patches:\n",
    "    # get_x pulls left or right; get_height pushes up or down\n",
    "    ax.text(i.get_x(), i.get_height() + 500000, str(round(i.get_height()/1000, 2)), rotation=90)\n",
    "\n",
    "ax.figure.savefig('/home/ubuntu/myplot.pdf')\n",
    "\n",
    "print_completion_time()\n",
    "# ax = posts.hist(column=\"Timestamp\", bins=15)\n",
    "# ax.figure.savefig('/home/ubuntu/myplot.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export posts by week\n",
    "\n",
    "Read posts from a given year(s) and save them into separate files by week number. This reduces size of individual files\n",
    "and simplifies later processing.\n",
    "\n",
    "In preparation, all posts from specific year are exported from PostgreSQL to a single `csv` file.\n",
    "\n",
    "Then, placeholder `csv` files are created with only the header row, 1 file per 1 week."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(53):\n",
    "    with open('/datadrive/train-data/posts_weeks/2016_posts_week_' + str(i) + '.csv', 'w+') as file:\n",
    "        file.write('IdPost;Author;Thread;Timestamp;Content;AuthorNumPosts;AuthorReputation;LastParse;parsed;Site;CitedPost;AuthorName;Likes;conv_time;week_no\\n')\n",
    "\n",
    "print_completion_time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Posts from given year are loaded in chunks and sorted by their individual publication date week number. The\n",
    "most obvious unwanted keywords are filtered out and dataframes are appended to their individual week files."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "df_2016 = pd.DataFrame()\n",
    "print(\"Started\")\n",
    "\n",
    "for chunk in pd.read_csv('/datadrive/posts_2016-18.csv', sep=';', header=0, chunksize=1000000):\n",
    "    print(\"Now processing chunk \" + str(i))\n",
    "    i += 1\n",
    "\n",
    "    # Initial filtering of most obvious key-words\n",
    "    chunk = chunk.dropna(subset=['Content'])\n",
    "    chunk = chunk[~chunk['Content'].str.contains('Drug')]\n",
    "    chunk = chunk[~chunk['Content'].str.contains('drug')]\n",
    "    chunk = chunk[~chunk['Content'].str.contains('pharmacy')]\n",
    "    chunk = chunk[~chunk['Content'].str.contains('Pharmacy')]\n",
    "\n",
    "\n",
    "    chunk['conv_time']=pd.to_datetime(chunk['Timestamp'], utc=True, infer_datetime_format=True)\n",
    "    print('\\nAppending')\n",
    "\n",
    "    # Append week number the dataframe.\n",
    "    chunk['week_no'] = chunk['conv_time'].apply(lambda x: datetime.date(x).isocalendar()[1])\n",
    "\n",
    "    chunk.dropna(subset=['week_no'], inplace=True)\n",
    "\n",
    "    for j in range(1, 53):\n",
    "        chunk.loc[chunk['week_no'] == j].to_csv(\n",
    "            path_or_buf=('/datadrive/train-data/posts_weeks/2016_posts_week_' + str(j) + '.csv'), mode='a', sep=';', header=False, index=False)\n",
    "\n",
    "    # df_2016 = df_2016.append(chunk, ignore_index=True)\n",
    "\n",
    "print_completion_time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Additional code (not used)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Start\")\n",
    "\n",
    "start_date = \"2016-01-01\"\n",
    "# stop_date = \"2016-12-31\"\n",
    "stop_date = \"2016-01-15\"\n",
    "\n",
    "start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "stop = datetime.strptime(stop_date, \"%Y-%m-%d\")\n",
    "\n",
    "start = pytz.utc.localize(start)\n",
    "stop = pytz.utc.localize(stop)\n",
    "\n",
    "location = '/datadrive/train-data/'\n",
    "\n",
    "while start < stop:\n",
    "    start = start + timedelta(days=1)  # increase day one by one\n",
    "\n",
    "    until = start + timedelta(days=30)\n",
    "\n",
    "    filename = os.path.join(location, start.strftime('%Y-%m-%d') + '_30.csv')\n",
    "    # print(start)\n",
    "    print(\"From: \" + str(start) + \" until: \" + str(until) + \": \" +\n",
    "          str(len(df_2016[(df_2016['conv_time'] > start) & (df_2016['conv_time'] < until)])))\n",
    "    df_2016[(df_2016['conv_time'] > start) & (df_2016['conv_time'] < until)].to_csv(\n",
    "        path_or_buf=filename, sep=';', index=False, mode='w+')\n",
    "\n",
    "print_completion_time()\n",
    "\n",
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "sentences = [\"The goal of this project is to obtain the token embedding from BERT's pre-trained model.\"]\n",
    "\n",
    "bert_embedding = BertEmbedding()\n",
    "result = bert_embedding(sentences)\n",
    "\n",
    "# print(result)\n",
    "print(len(result[0][1][19]))\n",
    "\n",
    "print_completion_time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}